# XLA Overview

XLA（加速线性代数）是用于优化TensorFlow计算的线性代数的域特定编译器。结果是在服务器和移动平台上的速度，内存使用率和可移植性得到了改善。最初，大多数用户不会从XLA中看到很大的好处，但是欢迎您通过使用即时（JIT）编译或提前编译（AOT）的XLA进行试验。特别鼓励开发者针对新硬件加速器尝试XLA。

XLA框架是实验性和积极的开发。特别是，尽管现有操作的语义不太可能发生变化，但预计将增加更多的操作来涵盖重要的用例。该团队欢迎来自社区的关于通过GitHub缺失功能和社区贡献的反馈。

## 为什么要构建XLA？

## XLA如何工作？

XLA的输入语言称为“HLO IR”，或称为HLO（高级优化程序）。操作语义页面描述了HLO的语义。将HLO视为编译器IR是最方便的。

XLA将HLO中定义的图形（“计算”）编译成各种体系结构的机器指令。XLA是模块化的，因为它很容易插入替代后端，以便定位一些新颖的硬件架构。用于x64和ARM64的CPU后端以及NVIDIA GPU后端均位于TensorFlow源代码树中。

下图显示了XLA中的编译过程：

![](https://github.com/erguixieshen/XLA/raw/master/week2/picture/2.png)

XLA带有多个与目标无关的优化和分析，如CSE，独立于目标的操作融合以及为计算分配运行时内存的缓冲区分析。

在独立于目标的步骤之后，XLA将HLO计算发送到后端。后端可以执行进一步的HLO级别分析和优化，这次是针对具体目标信息和需求。例如，XLA GPU后端可以执行专用于GPU编程模型的操作融合，并确定如何将计算划分为流。在这个阶段，后端也可以模式匹配某些操作或其组合来优化库调用。

下一步是目标特定的代码生成。XLA附带的CPU和GPU后端使用LLVM进行低级IR，优化和代码生成。这些后端以有效的方式发出代表XLA HLO计算所需的LLVM IR，然后调用LLVM从此LLVM IR发出本机代码。

GPU后端当前通过LLVM NVPTX后端支持NVIDIA GPU; CPU后端支持多个CPU ISA